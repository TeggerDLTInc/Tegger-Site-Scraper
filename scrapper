# -*- coding: utf-8 -*-
"""Composer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sEufD4yxUR_w_4SJZ7HzxXIFLSXJod2n
"""

!pip install \
 apache-airflow==1.10.12 \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.7.txt"

# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

## Import necessary models and operators
from airflow import models

## These are the operators we will use.
# bigquery_operator for all the manipulations in bigquerry
# python_operator when we require python (like building pipelines)

from airflow.contrib.operators import bigquery_operator
from airflow.operators import python_operator
from airflow.operators import DummyOperator

import datetime

default_dag_args = {
    'start_date': datetime.datetime(2020, 8, 20),
    'donot_pickle': True
}
    
# Define a model which runs once everyday
with models.DAG(
        'test_dag_pl2',
        schedule_interval=datetime.timedelta(days=1),
        default_args=default_dag_args) as dag:
    
    ## Start of daily task
    start = DummyOperator(task_id='start')

    # Takes in the last days data (under events_* table) 
    # Processes the data and does some feature engineering like getting session_id, base_page_location, etc
    # Then overwrites the 'yesterdays_data' with this data.
    
    firebase_yesterdays_data = bigquery_operator.BigQueryOperator(
         task_id='firebase_yesterdays_data',
         use_legacy_sql=False,
         bql="""
SELECT
  * EXCEPT(session_last_time,
    session_first_time),
  TIMESTAMP_DIFF(session_last_time, session_first_time, second) AS session_duration,
  EXTRACT(hour FROM session_first_time AT time zone 'Mexico/General') AS session_start_hour,
  time(session_first_time, 'Mexico/General') AS session_start_time,
  date(timestamp_micros(user_first_touch_timestamp), 'Mexico/General') AS user_first_visit,    CASE
    WHEN STRPOS(page_location, "?") > 0 THEN SUBSTR(page_location, 0, STRPOS(page_location, "?") - 1)
    WHEN STRPOS(page_location, "#") > 0 THEN SUBSTR(page_location, 0, STRPOS(page_location, "#") - 1)
    ELSE page_location
  END AS base_page_location 
 FROM (
  SELECT
    * EXCEPT(old_user_id,
      session_id,
      max_session),
    COALESCE(session_id + max_session,
      session_id) AS session_id,
    COUNT(*) OVER (PARTITION BY user_pseudo_id, session_id) AS num_events,
    SUM(CASE
        WHEN event_name = 'page_view' THEN 1
      ELSE
      0
    END
      ) OVER (PARTITION BY user_pseudo_id, session_id) AS page_views,
    MAX(TIMESTAMP_MICROS(event_timestamp)) OVER (PARTITION BY user_pseudo_id, session_id) AS session_last_time,
    MIN(TIMESTAMP_MICROS(event_timestamp)) OVER (PARTITION BY user_pseudo_id, session_id) AS session_first_time,
  FROM (
    SELECT
      * EXCEPT(session_start,
        ts,
        prev_evt_ts,
        is_session_start_event),
      MAX(is_session_start_event) OVER (PARTITION BY user_pseudo_id, session_id) AS has_session_start_event
    FROM (
      SELECT
        *,
        SUM(session_start) OVER (PARTITION BY user_pseudo_id ORDER BY ts) AS session_id
      FROM (
        SELECT
          *,
        IF
          (COALESCE(TIMESTAMP_DIFF(ts, prev_evt_ts, SECOND) / 60,
              1e10) >= 30,
            1,
            0) AS session_start
        FROM (
          SELECT
            *,
            TIMESTAMP_MICROS(event_timestamp) AS ts,
            LAG(TIMESTAMP_MICROS(event_timestamp)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp) AS prev_evt_ts,
          IF
            (event_name = "session_start",
              1,
              0) AS is_session_start_event
          FROM (
            SELECT
              * EXCEPT(unique_id)
            FROM (
              SELECT
                * EXCEPT(unique_id),
                event_table.unique_id AS unique_id
              FROM (
                SELECT
                  CONCAT(event_timestamp, user_pseudo_id, event_name) AS unique_id,
                  MAX(
                  IF
                    (user_properties_column = "user_id",
                      user_properties_value,
                      NULL)) AS user_id,
                  MAX(
                  IF
                    (user_properties_column = "gamification",
                      user_properties_value,
                      NULL)) AS gamification,
                  MAX(
                  IF
                    (user_properties_column = "displayName",
                      user_properties_value,
                      NULL)) AS displayName,
                  MAX(
                  IF
                    (user_properties_column = "tokenBalance",
                      user_properties_value,
                      NULL)) AS tokenBalance,
                  MAX(
                  IF
                    (user_properties_column = "email",
                      user_properties_value,
                      NULL)) AS email,
                  MAX(
                  IF
                    (user_properties_column = "interacts",
                      user_properties_value,
                      NULL)) AS interacts
                FROM (
                  WITH
                    sequences AS(
                      -- Combine the user_properties keys and value
                    SELECT
                      ARRAY(
                      SELECT
                        AS value CAST(CONCAT(key, '^', ifnull(value.string_value,
                              ''), ifnull(CAST(value.int_value AS string),
                              ''), ifnull(CAST(value.float_value AS string),
                              ''), ifnull(CAST(value.double_value AS string),
                              '')) AS string)
                      FROM
                        UNNEST(user_properties) ) AS user_properties,
                      event_timestamp,
                      user_id,
                      user_pseudo_id,
                      event_name
                    FROM
                      `tegger-prod.analytics_228863360.events_*`
                    WHERE
                      _table_suffix = REGEXP_REPLACE(CAST(DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY) AS string), r'\W+', '')
                      --
                      )
                  SELECT
                    sequences.event_timestamp,
                    sequences.user_id,
                    sequences.user_pseudo_id,
                    sequences.event_name,
                    SPLIT(flattened_user_properties, '^')[
                  OFFSET
                    (0)] user_properties_column,
                    SPLIT(flattened_user_properties, '^')[
                  OFFSET
                    (1)] user_properties_value
                  FROM
                    sequences
                  CROSS JOIN
                    UNNEST(sequences.user_properties) AS flattened_user_properties)
                WHERE
                  user_id IS NOT NULL
                GROUP BY
                  event_timestamp,
                  user_pseudo_id,
                  event_name) user_table
              RIGHT JOIN (
                SELECT
                  CONCAT(event_timestamp, user_pseudo_id, event_name) AS unique_id,
                  MAX(
                  IF
                    (event_params_column = "origin",
                      event_params_value,
                      NULL)) AS origin,
                  MAX(
                  IF
                    (event_params_column = "page_referrer",
                      event_params_value,
                      NULL)) AS page_referrer,
                  MAX(
                  IF
                    (event_params_column = "page_location",
                      event_params_value,
                      NULL)) AS page_location,
                  MAX(
                  IF
                    (event_params_column = "medium",
                      event_params_value,
                      NULL)) AS medium,
                  MAX(
                  IF
                    (event_params_column = "page_title",
                      event_params_value,
                      NULL)) AS page_title,
                  MAX(
                  IF
                    (event_params_column = "ga_session_number",
                      event_params_value,
                      NULL)) AS ga_session_number,
                  MAX(
                  IF
                    (event_params_column = "session_engaged",
                      event_params_value,
                      NULL)) AS session_engaged,
                  MAX(
                  IF
                    (event_params_column = "ga_session_id",
                      event_params_value,
                      NULL)) AS ga_session_id,
                  MAX(
                  IF
                    (event_params_column = "engaged_session_event",
                      event_params_value,
                      NULL)) AS engaged_session_event,
                  MAX(
                  IF
                    (event_params_column = "campaign",
                      event_params_value,
                      NULL)) AS campaign,
                  MAX(
                  IF
                    (event_params_column = "engagement_time_msec",
                      event_params_value,
                      NULL)) AS engagement_time_msec,
                  MAX(
                  IF
                    (event_params_column = "screenName",
                      event_params_value,
                      NULL)) AS screenName,
                  MAX(
                  IF
                    (event_params_column = "appName",
                      event_params_value,
                      NULL)) AS appName,
                  MAX(
                  IF
                    (event_params_column = "pathName",
                      event_params_value,
                      NULL)) AS pathName
                FROM (
                  WITH
                    sequences AS(
                      -- Combine the event_parameters and user_properties keys and value
                    SELECT
                      ARRAY(
                      SELECT
                        AS value CAST(CONCAT(key, '^', ifnull(value.string_value,
                              ''), ifnull(CAST(value.int_value AS string),
                              ''), ifnull(CAST(value.float_value AS string),
                              ''), ifnull(CAST(value.double_value AS string),
                              '')) AS string)
                      FROM
                        UNNEST(event_params) ) AS event_params,
                      event_timestamp,
                      user_pseudo_id,
                      user_id,
                      event_name
                      -- , row_number() OVER() unique_id
                    FROM
                      `tegger-prod.analytics_228863360.events_*`
                    WHERE
                      _table_suffix = REGEXP_REPLACE(CAST(DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY) AS string), r'\W+', '')
                      --
                      )
                  SELECT
                    sequences.event_timestamp,
                    sequences.user_pseudo_id,
                    sequences.event_name,
                    SPLIT(flattened_event_params, '^')[
                  OFFSET
                    (0)] event_params_column,
                    SPLIT(flattened_event_params, '^')[
                  OFFSET
                    (1)] event_params_value
                  FROM
                    sequences
                  CROSS JOIN
                    UNNEST(sequences.event_params) AS flattened_event_params)
                GROUP BY
                  event_timestamp,
                  user_pseudo_id,
                  event_name) event_table
              ON
                user_table.unique_id=event_table.unique_id) combined_table
            JOIN (
              SELECT
                CONCAT(event_timestamp, user_pseudo_id, event_name) AS unique_id,
                * EXCEPT(event_params,
                  user_properties,
                  user_id)
              FROM
                `tegger-prod.analytics_228863360.events_*`
              WHERE
                _table_suffix = REGEXP_REPLACE(CAST(DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY) AS string), r'\W+', '')) remaining_table
            ON
              remaining_table.unique_id = combined_table.unique_id )))) a
    LEFT JOIN (
      SELECT
        user_pseudo_id AS old_user_id,
        MAX(session_id) AS max_session
      FROM
        `tegger-prod.analytics_228863360.combined_until_28th_April`
      GROUP BY
        user_pseudo_id) b
    ON
      a.user_pseudo_id=b.old_user_id))"""
         ,
         bigquery_conn_id='bigquery_default',
   google_cloud_storage_conn_id='bigquery_default',
         destination_dataset_table='tegger-prod.analytics_228863360.yesterdays_data',
                                     write_disposition='WRITE_TRUNCATE',
                             create_disposition='CREATE_IF_NEEDED')
        
            
    # Selects all the distinct pages from the previous day.
    # Checks if the pages are present in the table 'additional_info' and select all the new pages.
    # Then performs web-scarping to extract information like article_content, section, etc 
    # and stores adds this information back to the table 'additional_info'.
    def python_func():
        import apache_beam as beam
        from bs4 import BeautifulSoup
        import requests
        from nltk.corpus import stopwords
        import nltk
        nltk.download('stopwords')
        import dateparser
        import string
        table = str.maketrans('', '', string.punctuation+'""')
        
        def get_info(row):
        
            try:
                response = requests.get(row['base_page_location'])
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Get the language used in the page. 'es' for spanish and 'en' for english.
                # Default language being Spanish
                try:
                    row['lang'] = soup.find('html')['lang'].lower()
                except:
                    row['lang'] = 'es'
                # Different processing based on the hostname
                if row['hostname'] == 'culturacolectiva.com':
        
                    # Extract the labels present at the end of each page.
                    # Returns empty string if the page does not have any tags
                    row['labels'] = ', '.join(tag.find('a').text for tag in soup.find_all("li", {"class": "tags__list-item"}))
            
                    # Searches for the tag containing class: article
                    article = soup.find("div", {"class": "article"})
        
                    if article is not None:
                        
                        # Try to get article content in either of the two ways
                        try: 
                            article_content = soup.find("article", {"class": "article__content article__new-content"}).text.strip('\n')
                        except: 
                            article_content = soup.find("article", {"class": "article__content mb-5"}).text
                            
                        # I wanted to run the spacy ner on the article content but I could not 
                        # download the model in the dataflow pipeline. The below lines are just the preprocessing steps.
                        article_content = article_content.lower() 
        
                        article_content = article_content.translate(table)
                        words = article_content.split()
        
                        important_words=[]
                        for word in words:
                            if row['lang'] == 'en' and word not in stopwords.words('english'):
                                important_words.append(word)                    
                            if row['lang'] == 'es' and word not in stopwords.words('spanish'):
                                important_words.append(word)
                        row['article_content'] = ' '.join(important_words)
                        
                        # Average readng speed can be 200 WPM but since I cleaned stopwords above. 
                        # I increased it to 100 WPM
                        row['est_time'] = len(row['article_content'].split())/100
        
                        tag = soup.find("div", {"class": "hero__author-info-block"})
                        if tag is None:
                            tag = soup.find("div", {"class": "hero__author"})
                            row['author_name'] = tag.find('span').text
                            row['date'] = str(dateparser.parse(tag.find('p', {'class' : "hero__date"}).text))
                        else: 
                            row['date'] = str(dateparser.parse(tag.find('span', 'hero__post-date').text))
                            row['author_name'] = tag.find_all('span')[1].text[:-3]
                        row['content_type'] = 'article'
        
                        try:
                            row['section'] = soup.find("span", {"class": "hero__category uppercase"}).text.strip()
                        except:
                            try:
                                row['section'] = row['page_title'].split('-')[-1].strip()
                            except: 
                                row['section'] = None
        
                    # Check if the link is a video
                    elif soup.find(True, {"class": "section__video-main"}) is not None and\
                        soup.find(True, {"class": "section__video-main"}).find('iframe') is not None:
                        # link = soup.find(True, {"class": "section__video-main"}).find('iframe')['src']
                        row['author_name'] = soup.find(True, {"class": "section__video-main"}).find(True, {"class": "content__category uppercase"}).text
                        row['date'] = str(dateparser.parse('20 March, 2000'))
                        row['article_content'] = None
                        row['content_type'] = 'video'
                        row['est_time'] = None
                        row['section'] = row['author_name']
                        
                    # If it's not the above two cases, assume it is browsing
                    else:
                        row['content_type'] = 'browsing'
                        row['author_name'] = None
                        row['date'] = str(dateparser.parse('20 March, 2000'))
                        row['article_content'] = None
                        row['est_time'] = None
                        row['section'] = None
        
                elif row['hostname'] == 'news.culturacolectiva.com':
                    row['labels'] = ', '.join(tag.text for tag in soup.find("div", {"class": "ccn-references-tags"}).find_all('a'))
                    tag = soup.find("div", {"class": "ccn-mc-artHd"})
        
                    if tag is None:
                        row['content_type'] = 'browsing'
                        row['author_name'] = None 
                        row['date'] = str(dateparser.parse('20 March, 2000'))
                        row['article_content'] = None
                        row['est_time'] = None
                    elif soup.find("div", {"class": "ccn-references-tags"}).find_all("a") != []\
                    and soup.find("div", {"class": "ccn-references-tags"}).find_all("a")[-1].text == 'videos':
                        row['author_name'] = tag.find('a').text
                        row['content_type'] = 'video'
                        row['date'] = str(dateparser.parse('20 March, 2000'))
                        row['article_content'] = None
                        row['est_time'] = None
                        row['section'] = row['author_name']
                    else:
                        row['author_name'] = tag.find('a').text
                        row['content_type'] = 'article'
                        try:
                            row['section'] = soup.find("a", {"class": "tag-category"}).text.strip()
                        except:
                            try:
                                if '|' in row['page_title']:
                                    row['section'] = row['page_title'].split('|')[-1].strip()
                                else:
                                    row['section'] = row['page_title'].split('-')[-1].strip()
                            except: 
                                row['section'] = None
                        row['date'] = str(dateparser.parse(tag.find('span').text))
                        article_content = soup.find('article').text.strip('\n')
                        article_content = article_content.lower() 
        
                        article_content = article_content.translate(table)
                        words = article_content.split()
        
                        important_words=[]
                        for word in words:
                            if row['lang'] == 'en' and word not in stopwords.words('english'):
                                important_words.append(word)                    
                            if row['lang'] == 'es' and word not in stopwords.words('spanish'):
                                important_words.append(word)
                        row['article_content'] = ' '.join(important_words)
                        row['est_time'] = len(row['article_content'].split())/100
        
                elif row['hostname'] == 'www.freim.tv':
                    try: row['labels'] = ' '.join([tag.text.translate(table).lower() for tag in soup.find("div", {"class": "tags"}).find_all('a')])
                    except: row['labels'] = ''
                    if len(soup.select('div[class*="single-title"]'))==0:
                        row['content_type'] = 'browsing'
                        row['author_name'] = None 
                        row['section'] = None
                        row['date'] = str(dateparser.parse('20 March, 2000'))
                        row['article_content'] = None
                    else:
                        try: row['section'] = soup.find(True, {"class": "category"}).find('a').text.strip()
                        except: 
                            try:
                                if '|' in row['page_title']:
                                    row['section'] = row['page_title'].split('|')[-1].strip()
                                elif '-' in row['page_title']:
                                    row['section'] = row['page_title'].split('-')[-1].strip()
                            except: 
                                row['section'] = None
                        try: row['author_name'] = soup.find(True, {"class": "author-info"}).find('a').text
                        except: row['author_name'] = None 
                        row['content_type'] = 'article'
                        try: row['date'] = str(dateparser.parse(soup.find(True, {"class": "date"}).text, date_formats=['%d/%m/%Y']))
                        except: row['date'] = str(dateparser.parse('20 March, 2000'))
                        article_content = ' '.join([paragraph.text for paragraph in soup.find_all('article')[0].find_all('p')])
                        # article_content = article_content.strip('\xa0').translate(table).lower()
        
                        # article_content = re.sub(r'\S*[^\w\s]\S*', '', article_content)
                        # words = article_content.split()
                        
                        important_words = article_content.split()
                        # if row['lang'] == 'en' and word not in stopwords.words('english'):
                        #     # for ent in nlp_en(article_content).ents:
                        #     important_words.append(ent)                    
                        # if row['lang'] == 'es' and word not in stopwords.words('spanish'):
                        #     # for ent in nlp_es(article_content).ents:
                        #     important_words.append(ent)
                        
                        row['est_time'] = len(important_words)/200
                        # important_words = [item for item, c in Counter(important_words).most_common() if item not in stopwords.words('spanish') and len(item)>3] 
                        row['article_content'] = article_content#' '.join(important_words[:30])#' '.join(important_words)
        
                else:
                    row['author_name'] = None
                    row['content_type'] = 'browsing'
                    row['section'] = None
                    row['labels'] = None
                    row['article_content'] = None
                    row['date'] = str(dateparser.parse('20 March, 2000'))
                    row['est_time'] = None
                    
            except: 
                row['author_name'] = None
                row['content_type'] = 'browsing'
                row['section'] = None
                row['labels'] = None
                row['article_content'] = None
                row['date'] = str(dateparser.parse('20 March, 2000'))
                row['est_time'] = None
                row['lang'] = 'es'
            yield row
            
        PROJECT='tegger-prod' 
        BUCKET='teggerbucket'
        
        query = """
SELECT
  a.base_page_location,
  MIN(a.hostname) AS hostname,
  MIN(a.page_title) AS page_title
FROM
  `tegger-prod.analytics_228863360.yesterdays_data` a
LEFT JOIN
  `tegger-prod.analytics_228863360.additional_info` b
ON
  a.base_page_location = b.base_page_location
WHERE
  b.date IS NULL
GROUP BY
  base_page_location"""
        

        OUTPUT_DIR = 'gs://{0}/test_case1'.format(BUCKET)
        REGION= 'us-east1'
        BUCKET='teggerbucket'

        options = {
    'staging_location': OUTPUT_DIR, 
    'temp_location': OUTPUT_DIR,
    'project': PROJECT,
    'region': REGION,
    'num_workers' : 24,
    'autoscaling_algorithm' : None
    }
        
        ## One of the reasons I am unable to run this on Composer is that it is not able to locate the 'requirements.txt' file
        # event though it is present in the given location.
        # I think this should run smoothly if this problem is resolved.
        
        opts = beam.pipeline.PipelineOptions(flags = ['--requirements_file=gs://teggerbucket/test_case1/requirements.txt'], **options)
        
        RUNNER = 'DataflowRunner'
    
        # Pipeline: First obtain the required table as specified in query,
        # Next, extract information for this table using the Python function.
        # Dump the information to the 'additional_info' table. 
        with beam.Pipeline(RUNNER, options = opts) as p:
            (p| 'read_bq' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))
        | 'compute_fit' >> beam.FlatMap(get_info)
        | 'write_bq' >> beam.io.gcp.bigquery.WriteToBigQuery('tegger-prod:analytics_228863360.additional_info', schema='base_page_location:STRING, hostname:STRING,\
 page_title:STRING, section:STRING, author_name:STRING, labels:STRING, content_type:STRING, article_content:STRING, est_time:FLOAT64, lang:STRING, date:DATETIME', \
                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\
                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))
    
    web_scraping = python_operator.PythonOperator(
        task_id='web_scraping',
        python_callable=python_func)
    
    ## Append the previous days data stored in 'yesterdays_data' to the 'combined_until_28th_April' table.
    join_yesterdays_and_combined = bigquery_operator.BigQueryOperator(
        task_id='join_tables',
        use_legacy_sql=False,
        bql="""
SELECT
  * 
    FROM
      `tegger-prod.analytics_228863360.yesterdays_data`
        ,
        bigquery_conn_id='bigquery_default',
  google_cloud_storage_conn_id='bigquery_default',
        destination_dataset_table='tegger-prod.analytics_228863360.combined_until_28th_April',
                                    write_disposition='WRITE_APPEND',
                            create_disposition='CREATE_IF_NEEDED')"""

    ## order in which the above tasks must be executed
    start >> firebase_yesterdays_data >> web_scraping >> join_yesterdays_and_combined
    print('done')

!pwd

!pip freeze --local > /content/colab_installed.txt

